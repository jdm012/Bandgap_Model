{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8339b1d",
   "metadata": {},
   "source": [
    "# FGSM Evaluation on Patolli and Concatenated Feature Spaces  \n",
    "\n",
    "This notebook investigates the impact of FGSM perturbations on model robustness, considering two input settings:  \n",
    "- Patolli descriptors alone  \n",
    "- A concatenated feature space combining CNN-extracted diffractogram features with Patolli descriptors  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5220bbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 11:33:29.844527: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-19 11:33:29.859518: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-19 11:33:29.863729: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-19 11:33:29.873719: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-19 11:33:30.534266: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486ae56c",
   "metadata": {},
   "source": [
    "Loading dataframes from the patolli-generated descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cedfc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1626, 1, 120) (1626, 4)\n"
     ]
    }
   ],
   "source": [
    "#Patolli-based descriptors only\n",
    "xtraval_patolli=np.load('data/patolli_generated_data/patolli_gen_desc_xtraval_2s_fgsm.npy')\n",
    "ytraval_patolli=np.load('data/patolli_generated_data/patolli_gen_desc_ytraval_2s_fgsm.npy')\n",
    "xtest_patolli=np.load('data/patolli_generated_data/patolli_gen_desc_xtest_2s_fgsm.npy')\n",
    "ytest_patolli=np.load('data/patolli_generated_data/patolli_gen_desc_ytest_2s_fgsm.npy')\n",
    "print(xtest_patolli.shape,ytest_patolli.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674271b",
   "metadata": {},
   "source": [
    "Loading concatenated dataframes (patolli + ccn-extracted descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1327491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load concatenated sets\n",
    "traval_concat_df=pd.read_csv(\"data/concatenated_data/traval_concat_df_2_fgsm.csv\")\n",
    "test_concat_df=pd.read_csv(\"data/concatenated_data/test_concat_df_2_fgsm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c8e9539",
   "metadata": {},
   "outputs": [],
   "source": [
    "bandgap_cols = [\"bg_gga\", \"bg_gga_opt\", \"bg_hse\", \"bg_hse_opt\"]\n",
    "# Extract targets from concatenated splits\n",
    "yconcat_traval = traval_concat_df[bandgap_cols].to_numpy()\n",
    "yconcat_test = test_concat_df[bandgap_cols].to_numpy()\n",
    "\n",
    "#print(yconcat_traval.shape, yconcat_test.shape)\n",
    "\n",
    "# Extract features only (drop cif + targets)\n",
    "xconcat_traval = traval_concat_df.drop(columns=[\"cif\"] + bandgap_cols).values\n",
    "xconcat_test = test_concat_df.drop(columns=[\"cif\"] + bandgap_cols).values\n",
    "\n",
    "#print(xconcat_traval.shape, xconcat_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfd50c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(losses, model_name, ytest, ptest):\n",
    "    # Plot the loss\n",
    "    plt.figure()\n",
    "    plt.plot(losses, label='Training')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch') \n",
    "    plt.legend()\n",
    "    plt.savefig(f'{model_name}/loss_mse_plot.png')\n",
    "    plt.close()\n",
    "    # Plot predicted vs real values\n",
    "    for item in range(ytest.shape[-1]):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(ytest[:,item],ptest[:,item], s=10, color='purple', alpha=0.5)\n",
    "        plt.grid()\n",
    "        plt.xlabel(r'actual band gap (eV)')\n",
    "        plt.ylabel(r'predicted bandgap (eV)')\n",
    "    #plt.plot(np.arange(-1,7), np.arange(-1,7), linewidth=1, color='black')\n",
    "        plt.title('Actual vs Predicted Values')\n",
    "    # Plot histograms of actual and predicted values\n",
    "        ytest_plot = ytest[:,item]\n",
    "        ptest_plot = ptest[:,item]\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(ytest_plot, bins=200, color='blue')\n",
    "        plt.hist(ptest_plot, bins=200, color='red', alpha=0.5)\n",
    "        plt.title('Histogram of Actual and Predicted Values')\n",
    "        plt.xlabel(r'band gap (eV)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend(['Actual', 'Predicted'], loc='upper right')\n",
    "    \n",
    "        plt.savefig(f'{model_name}/real_vs_predicted_{item}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    for item in range(ytest.shape[-1]):\n",
    "        residuals = ytest[:,item] - ptest[:,item]\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(residuals, bins=50, color='blue', alpha=0.5)\n",
    "        plt.title('Histogram of Actual and Predicted Values')\n",
    "        plt.xlabel('E_actual-E_predicted (eV)')\n",
    "        plt.ylabel('Frequency')\n",
    "    \n",
    "        plt.savefig(f'{model_name}/Distribution_{item}.png')\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0da0c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(xt, yt, neurons, blocks, drop):\n",
    "    # Input layer\n",
    "    inputs = Input(shape=xt.shape[1:])\n",
    "\n",
    "    # Initialize 'x' with the inputs\n",
    "    x = inputs\n",
    "    # Add 'num_blocks' blocks\n",
    "    for _ in range(blocks):\n",
    "        x = Dense(neurons)(x)\n",
    "        x = LayerNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(drop)(x)\n",
    "    # Output layer\n",
    "    outputs = Dense(yt.shape[-1], activation='linear')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4354144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model2(xt, yt, neurons, blocks, drop):\n",
    "    # Input layer\n",
    "    inputs = Input(shape=xt.shape[1:])\n",
    "\n",
    "    # Initialize 'x' with the inputs\n",
    "    x = inputs\n",
    "    x2= inputs\n",
    "    x2 = Dense(neurons)(x2)\n",
    "    x2 = LayerNormalization()(x2)\n",
    "    x2 = layers.Activation('relu')(x2)\n",
    "    x2 = Dropout(drop)(x2)\n",
    "    x2 = Dense(neurons)(x2)\n",
    "    x2 = LayerNormalization()(x2)\n",
    "    x2 = layers.Activation('relu')(x2)\n",
    "    x2 = Dropout(drop)(x2)\n",
    "    # Add 'num_blocks' blocks\n",
    "    for _ in range(blocks):\n",
    "        x = Dense(neurons)(x)\n",
    "        x = LayerNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(drop)(x)\n",
    "\n",
    "    x=layers.add([x,x2])\n",
    "    # Output layer\n",
    "    outputs = Dense(yt.shape[-1], activation='linear')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4200cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversary_fgsm_batch(model_fgsm, xconcat_traval, yconcat_traval, epsilon, loss_function, optim, batch_size, epochs):\n",
    "\n",
    "    num_batches = int(np.ceil(len(xconcat_traval) / batch_size))\n",
    "    all_losses = [] \n",
    "    epochs_loss_avg=[]\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print('Epoch', epoch)\n",
    "        epoch_losses = []\n",
    "        for batch in range(num_batches):\n",
    "            start_idx = batch * batch_size\n",
    "            end_idx = min((batch + 1) * batch_size, len(xconcat_traval))  # clamp to dataset size\n",
    "\n",
    "\n",
    "            x_batch_tf = tf.convert_to_tensor(xconcat_traval[start_idx:end_idx], dtype=tf.float32)\n",
    "            y_batch_tf = tf.convert_to_tensor(yconcat_traval[start_idx:end_idx], dtype=tf.float32)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(x_batch_tf)\n",
    "                pred = model_fgsm(x_batch_tf,training=True)\n",
    "                loss = loss_function(y_batch_tf, pred)\n",
    "\n",
    "            gradients = tape.gradient(loss, x_batch_tf)\n",
    "            signed_grad = tf.sign(gradients)\n",
    "            \n",
    "            x_batch_tf +=  epsilon * signed_grad\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred_perturbed = model_fgsm(x_batch_tf, training=True)\n",
    "                loss_perturbed= loss_function(y_batch_tf, pred_perturbed)\n",
    "\n",
    "            gradients_fgsm = tape.gradient(loss_perturbed, model_fgsm.trainable_variables)\n",
    "            optim.apply_gradients(zip(gradients_fgsm, model_fgsm.trainable_variables))\n",
    "\n",
    "            epoch_losses.append(loss_perturbed.numpy())\n",
    "\n",
    "        all_losses.append(epoch_losses)\n",
    "        df_loss = pd.DataFrame(np.asarray(all_losses).reshape((-1,1)))\n",
    "        df_loss.round(5).to_csv('losses.csv')\n",
    "        epoch_loss_avg = tf.reduce_mean(epoch_losses).numpy()\n",
    "        print('Average Epoch Loss (FGSM Model):', epoch_loss_avg)\n",
    "        epochs_loss_avg.append(epoch_loss_avg)\n",
    "    epochs_loss_avg= pd.Series(epochs_loss_avg, index=range(1, len(epochs_loss_avg) + 1))\n",
    "\n",
    "    return model_fgsm,epochs_loss_avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7eb6394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BandgapModel:\n",
    "    def __init__(self, neurons=64, blocks=3, drop=0.2, lr=0.001, version=1):\n",
    "        self.neurons = neurons\n",
    "        self.blocks = blocks\n",
    "        self.drop = drop\n",
    "        self.lr = lr\n",
    "        self.version = version\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.adv_history = None\n",
    "\n",
    "    def build(self, xtraval, ytraval):\n",
    "        if self.version == 1:\n",
    "            self.model = create_model(xtraval, ytraval, self.neurons, self.blocks, self.drop)\n",
    "        else:\n",
    "            self.model = create_model2(xtraval, ytraval, self.neurons, self.blocks, self.drop)\n",
    "        return self.model\n",
    "\n",
    "    def fit(self, xtraval, ytraval, validation_split=0.11, epoch=1, batch=1, verbose=2,\n",
    "            adversarial=False, epsilon=0.01):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Call build() before fit().\")\n",
    "\n",
    "        if adversarial:\n",
    "            optim = tf.keras.optimizers.Adam(self.lr)\n",
    "            loss_function = tf.keras.losses.LogCosh()\n",
    "            self.model, self.adv_history = adversary_fgsm_batch(\n",
    "                self.model, xtraval, ytraval, epsilon,\n",
    "                loss_function, optim, batch_size=batch, epochs=epoch\n",
    "            )\n",
    "            return self.adv_history\n",
    "        else:\n",
    "            self.history = self.model.fit(\n",
    "                xtraval, ytraval,\n",
    "                validation_split=validation_split,\n",
    "                batch_size=batch,\n",
    "                epochs=epoch,\n",
    "                verbose=verbose\n",
    "            )\n",
    "            return self.history\n",
    "\n",
    "\n",
    "    def evaluate_model(self,xtraval, ytraval, xtest, ytest, model_name,losses):\n",
    "\n",
    "        #history= model.fit(xtraval, ytraval, validation_split=0.11, epochs=epoch, batch_size=batch, verbose=2)\n",
    "\n",
    "        ptraval = self.model.predict(xtraval)\n",
    "        ptest = self.model.predict(xtest)\n",
    "\n",
    "        np.save(f'{model_name}/{model_name}_ptrav', ptraval)\n",
    "        np.save(f'{model_name}/{model_name}_ptest', ptest)\n",
    "\n",
    "        msetraval = mse(ytraval, ptraval)\n",
    "        msetest = mse(ytest, ptest)\n",
    "        \n",
    "        maetraval = mae(ytraval, ptraval)\n",
    "        maetest = mae(ytest, ptest)\n",
    "\n",
    "        # Calculate metrics\n",
    "        mae_nn = abs(ytest - ptest).mean(axis=0)\n",
    "        rmse_nn = ((ytest - ptest)**2).mean(axis=0)**0.5\n",
    "\n",
    "        with open(f'{model_name}/mse_mae.txt','a') as f:\n",
    "            f.write(model_name)\n",
    "            f.write(',')\n",
    "            f.write(\"%.5f\" % msetraval)\n",
    "            f.write(',')\n",
    "            f.write(\"%.5f\" % msetest)\n",
    "            f.write(',')\n",
    "            f.write(\"%.5f\" % maetraval)\n",
    "            f.write(',')\n",
    "            f.write(\"%.5f\" % maetest)\n",
    "            f.write('\\n\\n')\n",
    "            f.write(' mae:')\n",
    "            f.write(str(mae_nn))\n",
    "            f.write('\\n')\n",
    "            f.write(' rmse:')\n",
    "            f.write(str(rmse_nn))\n",
    "            f.close()\n",
    "        \n",
    "        self.model.save(f'{model_name}/{model_name}.keras')\n",
    "        losses.to_csv(f'{model_name}/losses.csv')\n",
    "        # Plot the model\n",
    "        plot_model(self.model, to_file=f'{model_name}/model.png', show_shapes=True, show_layer_names=False)\n",
    "        # Call the function to plot the graphs\n",
    "        plot_graphs(losses, model_name, ytest, ptest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7657c536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1758303212.093182    7657 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1758303212.138664    7657 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1758303212.138736    7657 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1758303212.143823    7657 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1758303212.143928    7657 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1758303212.143980    7657 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1758303212.314964    7657 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1758303212.315049    7657 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-09-19 11:33:32.315061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1758303212.315125    7657 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-09-19 11:33:32.315148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6061 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:0a:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Average Epoch Loss (FGSM Model): 0.52786475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1758303223.311674    7746 service.cc:146] XLA service 0x7f3ac8005040 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1758303223.311724    7746 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 2080, Compute Capability 7.5\n",
      "2025-09-19 11:33:43.317191: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-09-19 11:33:43.328726: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "I0000 00:00:1758303223.508441    7746 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Epoch 1\n",
      "Average Epoch Loss (FGSM Model): 0.5598509\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Epoch 1\n",
      "Average Epoch Loss (FGSM Model): 0.51569414\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Epoch 1\n",
      "Average Epoch Loss (FGSM Model): 0.537914\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Epoch 1\n",
      "Average Epoch Loss (FGSM Model): 0.5265959\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Epoch 1\n",
      "Average Epoch Loss (FGSM Model): 0.53188044\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Epoch 1\n",
      "Average Epoch Loss (FGSM Model): 0.55206466\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Epoch 1\n",
      "Average Epoch Loss (FGSM Model): 0.5583722\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Epoch 1\n",
      "Average Epoch Loss (FGSM Model): 0.5817469\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Epoch 1\n",
      "Average Epoch Loss (FGSM Model): 0.5940679\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning ranges\n",
    "lrs = [1e-4]\n",
    "batch_sizes = [32]\n",
    "epochs_list = [1]\n",
    "blocks_list = [1]\n",
    "drops = np.arange(0.0, 0.5, 0.1)\n",
    "neurons_list = range(2800, 2801, 280)\n",
    "epsilons = np.arange(0.115, 0.131, 0.015)  # epsilon sweep\n",
    "epsilons = np.around(epsilons, 3)\n",
    "\n",
    "# Full hyperparameter sweep\n",
    "for block, drop, batch, neurons, lr, epoch, epsilon in itertools.product(\n",
    "        blocks_list, drops, batch_sizes, neurons_list, lrs, epochs_list, epsilons):\n",
    "\n",
    "    # Initialize model\n",
    "    bandgap = BandgapModel(\n",
    "        neurons=neurons,\n",
    "        blocks=block,\n",
    "        drop=drop,\n",
    "        lr=lr,\n",
    "        version=1\n",
    "    )\n",
    "\n",
    "    # Build the model\n",
    "    bandgap.build(xconcat_traval, yconcat_traval)\n",
    "\n",
    "    # Train the model with adversarial FGSM\n",
    "    history = bandgap.fit(\n",
    "        xtraval=xconcat_traval,\n",
    "        ytraval=yconcat_traval,\n",
    "        validation_split=0.1,\n",
    "        epoch=epoch,\n",
    "        batch=batch,\n",
    "        verbose=2,\n",
    "        adversarial=True,\n",
    "        epsilon=epsilon  # pass current epsilon\n",
    "    )\n",
    "\n",
    "    # Create unique folder name including epsilon\n",
    "    model_name = datetime.datetime.now().strftime(\n",
    "        f\"%B_%d_%H%M%S_%Y_patolli_lr{lr}_neurons{neurons}_drop{drop}_batch{batch}_blocks{block}_epoch{epoch}_eps{epsilon}\"\n",
    "    )\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    # Evaluate model and save metrics, predictions, graphs, etc.\n",
    "    bandgap.evaluate_model(\n",
    "        xtraval=xconcat_traval,\n",
    "        ytraval=yconcat_traval,\n",
    "        xtest=xconcat_test,\n",
    "        ytest=yconcat_test,\n",
    "        model_name=model_name,\n",
    "        losses=history\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fc689bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef feature_importance_based_attack(x, model, epsilon=0.1):\\n  \"\"\"\\n  Performs a feature importance-based adversarial attack on tabular data.\\n\\n  Args:\\n      x: A 2D numpy array representing the tabular data (samples as rows, features as columns).\\n      model: A trained TensorFlow model for classification on tabular data.\\n      epsilon: The maximum magnitude of perturbation applied to each feature.\\n      target_class: The target class to misclassify the sample to (optional).\\n\\n  Returns:\\n      A 2D numpy array representing the adversarially perturbed data.\\n  \"\"\"\\n\\n  # Get the feature importance scores\\n  with tf.GradientTape() as tape:\\n    tape.watch(x)\\n    logits = model(x)\\n    loss = tf.keras.losses.sparse_categorical_crossentropy(tf.argmax(logits, axis=1), logits)\\n  gradients = tape.gradient(loss, x)\\n  importance_scores = abs(tf.reduce_mean(gradients, axis=0))  # Average gradient magnitude per feature\\n\\n  # Modify features with high importance\\n  modified_x = x.copy()\\n  num_features = x.shape[1]\\n  top_features = tf.math.top_k(importance_scores, k=int(0.2 * num_features))  # Modify top 20% features\\n\\n  for i in range(top_features.indices.shape[0]):\\n    feature_index = top_features.indices[i].numpy()\\n    sign = np.sign(gradients[0, feature_index])  # Use gradient sign for direction\\n    modified_x[:, feature_index] += sign * epsilon\\n\\n  # Clip modifications within epsilon range\\n  modified_x = tf.clip_by_value(modified_x, x - epsilon, x + epsilon)\\n\\n  return modified_x\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To check \n",
    "'''\n",
    "def feature_importance_based_attack(x, model, epsilon=0.1):\n",
    "  \"\"\"\n",
    "  Performs a feature importance-based adversarial attack on tabular data.\n",
    "\n",
    "  Args:\n",
    "      x: A 2D numpy array representing the tabular data (samples as rows, features as columns).\n",
    "      model: A trained TensorFlow model for classification on tabular data.\n",
    "      epsilon: The maximum magnitude of perturbation applied to each feature.\n",
    "      target_class: The target class to misclassify the sample to (optional).\n",
    "\n",
    "  Returns:\n",
    "      A 2D numpy array representing the adversarially perturbed data.\n",
    "  \"\"\"\n",
    "\n",
    "  # Get the feature importance scores\n",
    "  with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    logits = model(x)\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(tf.argmax(logits, axis=1), logits)\n",
    "  gradients = tape.gradient(loss, x)\n",
    "  importance_scores = abs(tf.reduce_mean(gradients, axis=0))  # Average gradient magnitude per feature\n",
    "\n",
    "  # Modify features with high importance\n",
    "  modified_x = x.copy()\n",
    "  num_features = x.shape[1]\n",
    "  top_features = tf.math.top_k(importance_scores, k=int(0.2 * num_features))  # Modify top 20% features\n",
    "\n",
    "  for i in range(top_features.indices.shape[0]):\n",
    "    feature_index = top_features.indices[i].numpy()\n",
    "    sign = np.sign(gradients[0, feature_index])  # Use gradient sign for direction\n",
    "    modified_x[:, feature_index] += sign * epsilon\n",
    "\n",
    "  # Clip modifications within epsilon range\n",
    "  modified_x = tf.clip_by_value(modified_x, x - epsilon, x + epsilon)\n",
    "\n",
    "  return modified_x\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2.17",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
